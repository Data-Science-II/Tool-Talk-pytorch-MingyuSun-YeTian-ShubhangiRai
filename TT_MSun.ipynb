{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import qiskit\n",
    "from qiskit import transpile, assemble\n",
    "from qiskit.visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concentrating on the first 100 samples\n",
    "n_samples = 100\n",
    "\n",
    "X_train = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                         transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Leaving only labels 0 and 1 \n",
    "idx = np.append(np.where(X_train.targets == 0)[0][:n_samples], \n",
    "                np.where(X_train.targets == 1)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True)\n",
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                        transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(X_test.targets == 0)[0][:n_samples], \n",
    "                np.where(X_test.targets == 1)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [5%]\tLoss: 0.2367\n",
      "Training [10%]\tLoss: 0.0145\n",
      "Training [15%]\tLoss: 0.0442\n",
      "Training [20%]\tLoss: 0.0047\n",
      "Training [25%]\tLoss: 0.0023\n",
      "Training [30%]\tLoss: 0.0015\n",
      "Training [35%]\tLoss: 0.0053\n",
      "Training [40%]\tLoss: 0.0007\n",
      "Training [45%]\tLoss: 0.0008\n",
      "Training [50%]\tLoss: 0.0001\n",
      "Training [55%]\tLoss: 0.0249\n",
      "Training [60%]\tLoss: 0.0050\n",
      "Training [65%]\tLoss: 0.0004\n",
      "Training [70%]\tLoss: 0.0008\n",
      "Training [75%]\tLoss: 0.0151\n",
      "Training [80%]\tLoss: 0.0050\n",
      "Training [85%]\tLoss: 0.0021\n",
      "Training [90%]\tLoss: 0.0004\n",
      "Training [95%]\tLoss: 0.0002\n",
      "Training [100%]\tLoss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__.\n",
    "# Every nn.Module subclass implements the operations on input data in the forward method.\n",
    "class Net_c(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_c, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6, kernel_size=5)\n",
    "        self.conv2=nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.dropout=nn.Dropout2d()\n",
    "        self.fc1=nn.Linear(256, 64)\n",
    "        self.fc2=nn.Linear(64,1)\n",
    "#         self.hybrid=Hybrid(qiskit.Aer.get_backend('qasm_simulator'), 100, np.pi / 2)\n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.conv1(x))\n",
    "        x=F.max_pool2d(x, 2)\n",
    "        x=F.relu(self.conv2(x))\n",
    "        x=F.max_pool2d(x,2)\n",
    "        x=self.dropout(x)\n",
    "        x=x.view(1,-1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "#         x=self.hybrid(x)\n",
    "        return torch.cat((x,1-x),-1)\n",
    "# To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call model.forward() directly!\n",
    "# create an instance of Net_c\n",
    "model_c = Net_c()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood)\n",
    "# initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.\n",
    "optimizer = optim.Adam(model_c.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "loss_list = []\n",
    "\n",
    "model_c.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#       reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model_c(data)\n",
    "        # Calculating loss\n",
    "        loss = loss_func(output, target)\n",
    "        # Backpropagate the prediction loss (deposits the gradients of the loss w.r.t. each parameter.)\n",
    "        loss.backward()\n",
    "        # Optimize the weights (adjust the parameters by the gradients collected in the backward pass.)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: 0.0001\n",
      "\tAccuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "model_c.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model_c(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n",
    "        sum(total_loss) / len(total_loss),\n",
    "        correct / len(test_loader) * 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation pi 0.53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐ ░ ┌───────────┐ ░ ┌─┐\n",
       "   q_0: ┤ H ├─░─┤ RY(theta) ├─░─┤M├\n",
       "        └───┘ ░ └───────────┘ ░ └╥┘\n",
       "meas: 1/═════════════════════════╩═\n",
       "                                 0 </pre>"
      ],
      "text/plain": [
       "        ┌───┐ ░ ┌───────────┐ ░ ┌─┐\n",
       "   q_0: ┤ H ├─░─┤ RY(theta) ├─░─┤M├\n",
       "        └───┘ ░ └───────────┘ ░ └╥┘\n",
       "meas: 1/═════════════════════════╩═\n",
       "                                 0 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement a hidden layer for our neural network using a parameterized quantum circuit\n",
    "# rotation angles for each gate are specified by the components of a classical input vector.\n",
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self._circuit=qiskit.QuantumCircuit(n_qubits)\n",
    "        all_qubits=[i for i in range(n_qubits)]\n",
    "        self.theta=qiskit.circuit.Parameter('theta')\n",
    "        self._circuit.h(all_qubits)\n",
    "        self._circuit.barrier()\n",
    "        self._circuit.ry(self.theta, all_qubits)\n",
    "        self._circuit.measure_all()\n",
    "        self.backend=backend\n",
    "        self.shots=shots\n",
    "    def run(self, thetas):\n",
    "        t_qc=transpile(self._circuit,self.backend)\n",
    "        qobj=assemble(t_qc, shots=self.shots,parameter_binds=[{self.theta:theta} for theta in thetas])\n",
    "        job=self.backend.run(qobj)\n",
    "        result=job.result().get_counts(self._circuit)\n",
    "        counts=np.array(list(result.values()))\n",
    "        states=np.array(list(result.keys())).astype(float)\n",
    "        probabilities=counts/self.shots\n",
    "        expectation=np.sum(states*probabilities)\n",
    "        return np.array([expectation])\n",
    "simulator = qiskit.Aer.get_backend('qasm_simulator')\n",
    "\n",
    "circuit = QuantumCircuit(1, simulator, 100)\n",
    "print('Expected value for rotation pi {}'.format(circuit.run([np.pi])[0]))\n",
    "circuit._circuit.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [5%]\tLoss: -0.8237\n",
      "Training [10%]\tLoss: -0.9231\n",
      "Training [15%]\tLoss: -0.9437\n",
      "Training [20%]\tLoss: -0.9469\n",
      "Training [25%]\tLoss: -0.9453\n",
      "Training [30%]\tLoss: -0.9557\n",
      "Training [35%]\tLoss: -0.9613\n",
      "Training [40%]\tLoss: -0.9729\n",
      "Training [45%]\tLoss: -0.9720\n",
      "Training [50%]\tLoss: -0.9786\n",
      "Training [55%]\tLoss: -0.9738\n",
      "Training [60%]\tLoss: -0.9787\n",
      "Training [65%]\tLoss: -0.9820\n",
      "Training [70%]\tLoss: -0.9794\n",
      "Training [75%]\tLoss: -0.9866\n",
      "Training [80%]\tLoss: -0.9877\n",
      "Training [85%]\tLoss: -0.9893\n",
      "Training [90%]\tLoss: -0.9897\n",
      "Training [95%]\tLoss: -0.9919\n",
      "Training [100%]\tLoss: -0.9929\n"
     ]
    }
   ],
   "source": [
    "class HybridFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input,quantum_circuit, shift):\n",
    "        ctx.shift=shift\n",
    "        ctx.quantum_circuit=quantum_circuit\n",
    "        expectation_z=ctx.quantum_circuit.run(input[0].tolist())\n",
    "        result=torch.tensor([expectation_z])\n",
    "        ctx.save_for_backward(input, result)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, expectation_z=ctx.saved_tensors\n",
    "        input_list=np.array(input.tolist())\n",
    "        shift_right=input_list+np.ones(input_list.shape)*ctx.shift\n",
    "        shift_left=input_list-np.ones(input_list.shape)*ctx.shift\n",
    "        gradients=[]\n",
    "        for i in range(len(input_list)):\n",
    "            expectation_right=ctx.quantum_circuit.run(shift_right[i])\n",
    "            expectation_left=ctx.quantum_circuit.run(shift_left[i])\n",
    "            gradient=torch.tensor([expectation_right])-torch.tensor([expectation_left])\n",
    "            gradients.append(gradient)\n",
    "        gradients=np.array([gradients]).T\n",
    "        return torch.tensor([gradients]).float()*grad_output.float(), None, None\n",
    "class Hybrid(nn.Module):\n",
    "    def __init__(self, backend, shots, shift):\n",
    "        super(Hybrid, self).__init__()\n",
    "        self.quantum_circuit=QuantumCircuit(1, backend, shots)\n",
    "        self.shift=shift\n",
    "    def forward(self, input):\n",
    "        return HybridFunction.apply(input, self.quantum_circuit, self.shift)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6, kernel_size=5)\n",
    "        self.conv2=nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.dropout=nn.Dropout2d()\n",
    "        self.fc1=nn.Linear(256, 64)\n",
    "        self.fc2=nn.Linear(64,1)\n",
    "        self.hybrid=Hybrid(qiskit.Aer.get_backend('qasm_simulator'), 100, np.pi / 2)\n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.conv1(x))\n",
    "        x=F.max_pool2d(x, 2)\n",
    "        x=F.relu(self.conv2(x))\n",
    "        x=F.max_pool2d(x,2)\n",
    "        x=self.dropout(x)\n",
    "        x=x.view(1,-1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        x=self.hybrid(x)\n",
    "        return torch.cat((x,1-x),-1)\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "epochs = 20\n",
    "loss_list = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        # Calculating loss\n",
    "        loss = loss_func(output, target)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -0.9870\n",
      "\tAccuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n",
    "        sum(total_loss) / len(total_loss),\n",
    "        correct / len(test_loader) * 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
